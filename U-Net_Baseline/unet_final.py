# -*- coding: utf-8 -*-
"""UNet_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U-cCUgssS2DFQTmjw_znF-QVJ_ljkR7w
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

# Hyperparameters
BATCH_SIZE  = 1
TARGET_SIZE = (224, 224)
EPOCHS      = 50  # adjust as needed

import os, glob
import numpy as np

# point these at your two folders
INPUT_DIR  = '/content/drive/MyDrive/datasets/dataset_train/inputs'
OUTPUT_DIR = '/content/drive/MyDrive/datasets/dataset_train/target'

def load_npy_from_dir(dir_path):
    # grab all .npy files (non-recursive); adjust pattern if you have subfolders
    pattern = os.path.join(dir_path, '*.npy')
    files = sorted(glob.glob(pattern))
    if not files:
        raise ValueError(f"No .npy files found in {dir_path}")
    # load & stack into (N, …) array
    return np.stack([np.load(f).astype('float32') for f in files], axis=0)

# usage
power_maps = load_npy_from_dir(INPUT_DIR)
temp_maps  = load_npy_from_dir(OUTPUT_DIR)

# if they differ in count, trim to the smaller one
n = min(power_maps.shape[0], temp_maps.shape[0])
power_maps = power_maps[:n]
temp_maps  = temp_maps[:n]

print("Loaded shapes:", power_maps.shape, temp_maps.shape)

# Slice channel 0 → shape becomes (N,224,224,1)
X = power_maps[..., 0:1]
Y = temp_maps[..., 0:1]

print("power_maps shape before creating dataset:", X.shape)
print("temp_maps shape before creating dataset:", Y.shape)

# 80% train / 20% test split
X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42
)

print(f"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}")

def make_ds(X, Y, batch_size=BATCH_SIZE, target_size=TARGET_SIZE):
    ds = tf.data.Dataset.from_tensor_slices((X, Y))
    ds = ds.map(
        lambda x, y: (
            tf.image.resize(x, target_size),
            tf.image.resize(y, target_size)
        ),
        num_parallel_calls=tf.data.AUTOTUNE
    )
    return ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)

train_ds = make_ds(X_train, Y_train)
test_ds  = make_ds(X_test,  Y_test)

from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Conv2DTranspose, Concatenate
from tensorflow.keras.models import Model

def build_unet(input_shape=(224,224,1)):
    inputs = Input(shape=input_shape)
    # Encoder
    c1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    c1 = Conv2D(64, 3, activation='relu', padding='same')(c1)
    p1 = MaxPooling2D()(c1)

    c2 = Conv2D(128, 3, activation='relu', padding='same')(p1)
    c2 = Conv2D(128, 3, activation='relu', padding='same')(c2)
    p2 = MaxPooling2D()(c2)

    # Bottleneck
    c5 = Conv2D(512, 3, activation='relu', padding='same')(p2)
    c5 = Conv2D(512, 3, activation='relu', padding='same')(c5)

    # Decoder
    u6 = Conv2DTranspose(256, 2, strides=2, padding='same')(c5)
    merge6 = Concatenate()([u6, c2])
    c6 = Conv2D(256, 3, activation='relu', padding='same')(merge6)
    c6 = Conv2D(256, 3, activation='relu', padding='same')(c6)

    u7 = Conv2DTranspose(128, 2, strides=2, padding='same')(c6)
    merge7 = Concatenate()([u7, c1])
    c7 = Conv2D(128, 3, activation='relu', padding='same')(merge7)
    c7 = Conv2D(128, 3, activation='relu', padding='same')(c7)

    outputs = Conv2D(1, 1, activation='linear')(c7)
    return Model(inputs, outputs)

model = build_unet(input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 1))
model.summary()

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=[tf.keras.metrics.MeanAbsoluteError(name='mae')]
)

import os
ckpt_dir = '/content/drive/MyDrive/TM_checkpoints'
os.makedirs(ckpt_dir, exist_ok=True)
ckpt_path = os.path.join(ckpt_dir, 'best_model.h5')

# 3) Callback to save only the best model (by val_mae)
checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(
    filepath=ckpt_path,
    monitor='val_mae',
    mode='min',
    save_best_only=True,
    verbose=1
)

import time
class TimeHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs=None):
        self.epoch_times = []
    def on_epoch_begin(self, epoch, logs=None):
        self._start_time = time.time()
    def on_epoch_end(self, epoch, logs=None):
        duration = time.time() - self._start_time
        self.epoch_times.append(duration)
        thru = X_train.shape[0] / duration
        print(f"→ Epoch {epoch:02d} took {duration:.2f}s, throughput {thru:.1f} samples/s")

time_cb = TimeHistory()

history = model.fit(
    train_ds,
    epochs=EPOCHS,
    validation_data=test_ds,
    callbacks=[checkpoint_cb, time_cb]
)

loss, mae = model.evaluate(test_ds)
print(f"Test loss (MSE): {loss:.4f}")
print(f"Test MAE   : {mae:.4f}")

import numpy as np

# 1) get raw predictions and ground-truth from the test set
y_pred = model.predict(test_ds)
y_true = np.concatenate([y for _, y in test_ds], axis=0)

# 2) compute mean absolute error in °C
abs_loss = np.mean(np.abs(y_pred - y_true))
print(f"Mean Absolute Error: {abs_loss:.4f} °C")

import time

# a) Warm-up
for x_batch, _ in test_ds.take(1):
    _ = model.predict(x_batch)

# b) Measure over the entire test set
start = time.time()
n = 0
for x_batch, _ in test_ds:
    n += x_batch.shape[0]
    _ = model.predict(x_batch)
elapsed = time.time() - start

throughput = n / elapsed
latency_ms = elapsed / n * 1000

print(f"Test set: {n} samples in {elapsed:.2f}s")
print(f"→ Throughput: {throughput:.2f} samples/s")
print(f"→ Avg latency: {latency_ms:.2f} ms/sample")

# A quick rule-of-thumb: ~2 × total trainable parameters
total_params = model.count_params()
flops_approx = 2 * total_params
print(f"Approximate FLOPs per inference: {flops_approx:,}")

import matplotlib.pyplot as plt

# Take one batch from the test dataset and plot input, truth, and prediction
for x_batch, y_batch in test_ds.take(1):
    # Run the model to get the prediction
    y_pred_batch = model.predict(x_batch)

    # Extract the single sample and its single channel
    input_power = x_batch[0, ..., 0]
    true_temp   = y_batch[0, ..., 0]
    pred_temp   = y_pred_batch[0, ..., 0]

    # Plot side-by-side
    fig, axes = plt.subplots(1, 3, figsize=(12, 4))
    for ax, img, title in zip(
        axes,
        [input_power, true_temp, pred_temp],
        ['Input Power', 'True Temp', 'Predicted Temp']
    ):
        ax.imshow(img, cmap='magma' if 'Power' in title else 'hot')
        ax.set_title(title)
        ax.axis('off')
    plt.tight_layout()
    plt.show()
    break

# --- adjust these paths to your test folders ---
TEST_INPUT_DIR  = '/content/drive/MyDrive/datasets/dataset_test/inputs'
TEST_TARGET_DIR = '/content/drive/MyDrive/datasets/dataset_test/targets'

# reuse your directory-loading helper (or re-define it here)
import os, glob
import numpy as np

def load_npy_from_dir(dir_path, recursive=False):
    pattern = os.path.join(dir_path, '**', '*.npy') if recursive else os.path.join(dir_path, '*.npy')
    files = sorted(glob.glob(pattern, recursive=recursive))
    if not files:
        raise ValueError(f"No .npy files found in {dir_path}")
    return np.stack([np.load(f).astype('float32') for f in files], axis=0)

# load test arrays
x_test = load_npy_from_dir(TEST_INPUT_DIR, recursive=True)   # set recursive=True if you have sub-folders
y_test = load_npy_from_dir(TEST_TARGET_DIR, recursive=True)

x_test = x_test[..., 0:1]
y_test = y_test[..., 0:1]


# trim in case counts differ
n = min(len(x_test), len(y_test))
x_test, y_test = x_test[:n], y_test[:n]

print(f"Test set shapes —  x: {x_test.shape},  y: {y_test.shape}")

# --- Evaluate the model on the full test set ---
# assumes your UNet model is called `model`
results = model.evaluate(x_test, y_test, batch_size=4)     # tweak batch_size as needed
print("Test loss / metrics:", results)

# --- (Optional) Run per‐sample inference and visualize a few results ---
import matplotlib.pyplot as plt

preds = model.predict(x_test, batch_size=4)
for i in range(min(5, n)):
    fig, axes = plt.subplots(1,3, figsize=(12,4))
    axes[0].imshow(x_test[i,...,0],  cmap='inferno')
    axes[0].set_title("Input power")
    axes[1].imshow(y_test[i,...,0], cmap='inferno')
    axes[1].set_title("Ground truth")
    axes[2].imshow(preds[i,...,0], cmap='inferno')
    axes[2].set_title("Prediction")
    for ax in axes: ax.axis('off')
    plt.show()

import os
import glob
import numpy as np
from collections import defaultdict

# ─── MOUNTED DRIVE PATH ────────────────────────────────────────────────────────
drive_root   = "/content/drive/MyDrive"
results_root = os.path.join(drive_root, "AI_Temperature", "RESULTS_DAC")
gt_save_dir   = os.path.join(results_root, "gt")
pred_save_dir = os.path.join(results_root, "pred")
os.makedirs(gt_save_dir,   exist_ok=True)
os.makedirs(pred_save_dir, exist_ok=True)

# ─── USER CONFIG ───────────────────────────────────────────────────────────────
# (assumes `model` is already built & loaded above)
inp_dir    = "/content/drive/MyDrive/datasets/dataset_test/inputs"
gt_dir     = "/content/drive/MyDrive/datasets/dataset_test/targets"
heat_value = 950.0
prefixes   = ['multi_gpu', 'ascend910', 'micro150']
# ────────────────────────────────────────────────────────────────────────────────

results = defaultdict(lambda: {'mae': [], 'rmse': [], 'r2': []})

def eval_sample_keras(model, inp_np, gt_np, heat_value):
    # Use only the first channel of the input as the model expects a single channel
    inp = inp_np[..., 0:1].astype(np.float32)

    x    = np.expand_dims(inp, axis=0)                  # shape (1, H, W, 1)
    pred = model.predict(x)[0,...]                     # shape (H, W) or (H, W, 1)
    if pred.ndim == 3 and pred.shape[-1] == 1:
        pred = pred[...,0]

    gt = gt_np.astype(np.float32)
    if gt.ndim == 3 and gt.shape[-1] == 1:
        gt = gt[...,0]
    # Also use only the first channel of the ground truth
    gt = gt_np[..., 0].astype(np.float32)


    mae   = np.mean(np.abs(pred - gt))
    rmse  = np.sqrt(np.mean((pred - gt)**2))
    ss_res= np.sum((pred - gt)**2)
    ss_tot= np.sum((gt   - gt.mean())**2)
    r2    = 1 - ss_res/ss_tot if ss_tot > 0 else np.nan

    return pred, gt, mae, rmse, r2

# ─── EVALUATION LOOP ────────────────────────────────────────────────────────────
for inp_path in sorted(glob.glob(os.path.join(inp_dir, "*.npy"))):
    base = os.path.basename(inp_path)[:-4]
    prefix = next((p for p in prefixes if base.startswith(p)), None)
    if prefix is None:
        continue

    # swap "_inp" → "_out" to match your GT filenames
    gt_filename = base.replace("_inp", "_out") + ".npy"
    gt_path     = os.path.join(gt_dir, gt_filename)
    if not os.path.exists(gt_path):
        print(f"⚠️ missing GT for {base} (looking for {gt_filename})")
        continue

    inp_np = np.load(inp_path)
    gt_np  = np.load(gt_path)
    pred, gt, mae, rmse, r2 = eval_sample_keras(model, inp_np, gt_np, heat_value)

    # save to Drive
    np.save(os.path.join(gt_save_dir,   gt_filename), gt)
    pred_filename = base.replace("_inp", "_pred") + ".npy"
    np.save(os.path.join(pred_save_dir, pred_filename), pred)

    results[prefix]['mae'].append(mae)
    results[prefix]['rmse'].append(rmse)
    results[prefix]['r2'].append(r2)

# ─── PRINT RESULTS ──────────────────────────────────────────────────────────────
print("\n=== Average metrics by prefix ===")
for p in prefixes:
    if results[p]['mae']:
        print(f"{p:12s} | MAE: {np.mean(results[p]['mae']):.4f}  "
              f"RMSE: {np.mean(results[p]['rmse']):.4f}  "
              f"R²: {np.mean(results[p]['r2']):.4f}")
    else:
        print(f"{p:12s} | no samples found")

all_mae  = np.hstack([results[p]['mae']  for p in prefixes if results[p]['mae']])
all_rmse = np.hstack([results[p]['rmse'] for p in prefixes if results[p]['rmse']])
all_r2   = np.hstack([results[p]['r2']   for p in prefixes if results[p]['r2']])
print(f"\nOverall   | MAE: {all_mae.mean():.4f}  "
      f"RMSE: {all_rmse.mean():.4f}  "
      f"R²: {all_r2.mean():.4f}")